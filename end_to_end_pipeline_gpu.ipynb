{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIpFDprmuIyB"
      },
      "source": [
        "1. Collect all the grievances, clean them, Low Priority: try to identify the correct encoding(top6) by removing the junk characters, that is, characters not from those 9 languages\n",
        "\n",
        "2.  Do rewording and segmentation\n",
        "\n",
        "3. Convert all the grievances to common language that is English, need to figure out how to identify the language so as to specify to the model, should be done pretty soon\n",
        "\n",
        "4. Calculate the time it takes to translate all the non roman grievances to english, try different token lengths 64, 128, 512, etc\n",
        "\n",
        "5. Build an HNSW index on the top of this to calculate the nearby grievances and calculate the time to build the index\n",
        "6. Build a system to take a query and show the results using hnsw index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4ezmqphuIyC"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "from math import log\n",
        "import os\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "import time\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
        "import torch\n",
        "import hnswlib\n",
        "import langid\n",
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JF0iI1MbuIyJ"
      },
      "outputs": [],
      "source": [
        "with open('stratified_sampled.pickle', 'rb') as file:\n",
        "\n",
        "    # use the pickle.dump() method to save the object to the file\n",
        "    sampled_df = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhBFb-GluIyJ"
      },
      "outputs": [],
      "source": [
        "sampled_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MM_m1gpvuIyJ"
      },
      "outputs": [],
      "source": [
        "sampled_df.Language.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DW5cY2oEwezK"
      },
      "outputs": [],
      "source": [
        "#  !pip install sentencepiece transformers langid hnswlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1CqW2rsxpNn"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIarC0ZcuIyJ"
      },
      "outputs": [],
      "source": [
        "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-one-mmt\").to(device)\n",
        "# tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-one-mmt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hng0gfDV0l1v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fmzlbe_n0Wrs"
      },
      "outputs": [],
      "source": [
        "# device = \"cuda\"\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-one-mmt\")\n",
        "# tokenizer.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8tfNEiFuIyJ"
      },
      "source": [
        "### facebook/mbart-large-50-many-to-one-mmt Size = 2.44G"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTk1IkrjuIyJ"
      },
      "outputs": [],
      "source": [
        "lang_to_code = dict()\n",
        "lang_to_code['Hindi'] = \"hi_IN\"\n",
        "lang_to_code['Bengali'] = \"bn_IN\"\n",
        "lang_to_code['Gujarati'] = \"gu_IN\"\n",
        "lang_to_code['Telugu'] = \"te_IN\"\n",
        "lang_to_code['Tamil'] = \"ta_IN\"\n",
        "lang_to_code['Kannada'] = \"kn_IN\"\n",
        "lang_to_code['Odia'] = \"or_IN\"\n",
        "lang_to_code['Punjabi'] = \"pa_IN\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qvm5eKdVuIyJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.cuda.amp import autocast\n",
        "\n",
        "# @torch.no_grad()\n",
        "# def translate(input_texts, lang):\n",
        "#     if lang == \"English\":\n",
        "#         return input_texts\n",
        "\n",
        "#     tokenizer.src_lang = lang_to_code[lang]\n",
        "#     encoded_ar = tokenizer.batch_encode_plus(input_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    \n",
        "#     with autocast():\n",
        "#         generated_tokens = model.generate(**encoded_ar)\n",
        "    \n",
        "#     decoded_texts = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "#     return decoded_texts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LHHDXLZ0uR0"
      },
      "outputs": [],
      "source": [
        "# def translate(input_text, lang):\n",
        "#     if lang == \"English\":\n",
        "#         return input_text\n",
        "\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-one-mmt\")\n",
        "#     tokenizer.src_lang = lang_to_code[lang]\n",
        "#     # tokenizer.tgt_lang = lang_to_code[\"en\"]\n",
        "#     # tokenizer.to(device)\n",
        "\n",
        "#     encoded_ar = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "#     generated_tokens = model.generate(encoded_ar.to(device))\n",
        "#     decoded_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
        "#     return decoded_text.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRC8LFNg428o"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSZyefXN426P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-5t9n25423-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.cuda.amp import autocast\n",
        "\n",
        "@torch.no_grad()\n",
        "def translate_batch(input_texts, lang, batch_size):\n",
        "    if lang == \"English\":\n",
        "        return input_texts\n",
        "\n",
        "    tokenizer.src_lang = lang_to_code[lang]\n",
        "    decoded_texts = []\n",
        "    for i in range(0, len(input_texts), batch_size):\n",
        "        input_batch = input_texts[i:i+batch_size]\n",
        "        encoded_ar = tokenizer(input_batch, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
        "        \n",
        "        with autocast():\n",
        "            generated_tokens = model.generate(**encoded_ar)\n",
        "        \n",
        "        decoded_batch = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "        decoded_texts.extend(decoded_batch)\n",
        "    \n",
        "    return decoded_texts\n",
        "\n",
        "# Group the dataframe by language\n",
        "grouped_df = sampled_df.groupby('Language')\n",
        "\n",
        "start = time.time()\n",
        "# Translate each group of texts in batches\n",
        "translated_texts = []\n",
        "for lang, group in grouped_df:\n",
        "    input_texts = group['subject_content_cleaned'].tolist()\n",
        "    translated_batch = translate_batch(input_texts, lang, batch_size=32)  # adjust batch size as needed\n",
        "    translated_texts.extend(translated_batch)\n",
        "\n",
        "# Add the translated texts back to the dataframe\n",
        "sampled_df['translated_text'] = translated_texts\n",
        "duration = time.time() - start\n",
        "print(\"Translation time per grievance:\", str(duration/len(sampled_df)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWsakSBe42vY"
      },
      "outputs": [],
      "source": [
        "# !pip install -U sentence-transformers==1.2.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qF43BsDeOmnM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_salesken = SentenceTransformer(\"salesken/similarity-eng-hin_latin\").to(device)\n",
        "\n",
        "batch_size = 128\n",
        "embeddings = []\n",
        "\n",
        "start = time.time()\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(sampled_df), batch_size):\n",
        "        batch = sampled_df['translated_text'][i:i+batch_size].tolist()\n",
        "        batch_embeddings = model_salesken.encode(batch, device=device)\n",
        "        batch_embeddings = torch.from_numpy(batch_embeddings).to(device)\n",
        "        embeddings.append(batch_embeddings)\n",
        "\n",
        "embeddings = torch.cat(embeddings)\n",
        "print(\"Embedding time per grievance in seconds: \", str((time.time()-start)/len(sampled_df)))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Total translation plus embedding time for 1 grievance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "round(0.006974324837073937 + 0.08459546515991638, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RevIRvCMQuD6"
      },
      "outputs": [],
      "source": [
        "import hnswlib\n",
        "\n",
        "# Define the index parameters\n",
        "index_size = len(embeddings)\n",
        "embedding_dim = embeddings.shape[1]\n",
        "index = hnswlib.Index(space='cosine', dim=embedding_dim)\n",
        "\n",
        "# Initialize the index\n",
        "index.init_index(max_elements=index_size, ef_construction=200, M=64)\n",
        "\n",
        "# Add the embeddings to the index\n",
        "index.add_items(embeddings.cpu().numpy())\n",
        "\n",
        "# Set the index to be search-ready\n",
        "index.set_ef(50)\n",
        "\n",
        "# Save the index to disk\n",
        "index.save_index('index.hnsw')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8h5R1u6uIyP"
      },
      "outputs": [],
      "source": [
        "# # Initialize the HNSW index\n",
        "# index = hnswlib.Index(space='l2', dim=embeddings[0].shape[0])\n",
        "# index.init_index(max_elements=len(embeddings), ef_construction=100, M=64)\n",
        "\n",
        "# # Add embeddings to the index\n",
        "# for i, emb in enumerate(embeddings):\n",
        "#     index.add_items(emb, i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpVAfpNYuIyQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Query the index\n",
        "query = 'corona killed many people'\n",
        "query_embedding = model_salesken.encode(query)\n",
        "labels, distances = index.knn_query(query_embedding, k=20)\n",
        "\n",
        "\n",
        "# Print top 5 most similar documents\n",
        "for i, label in enumerate(labels[0]):\n",
        "    print(f'Top {i+1} document: {sampled_df.iloc[label][\"subject_content_cleaned\"]}, distance: {distances[0][i]}')\n",
        "    print()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Translation time per grievance: 0.06363993638998026\n",
        "\n",
        "Embedding time per grievance in seconds:  0.007699702288601901"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.07133963867858216"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "0.06363993638998026 + 0.007699702288601901"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "ps",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "db07cf3586e4a3d517105e23490166b0186d57892d50b74d9e1d6eda510229fd"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
