{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/joker/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from math import log\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"/Users/joker/Library/CloudStorage/OneDrive-IITKanpur/Thesis/data_100k.csv\")\n",
    "df = df.loc[:, ['description']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = str(text).lower() \n",
    "    text = re.sub(r\"\\[(.*?)\\]\", \"\", text)  \n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove multiple spaces in content\n",
    "    text = re.sub(r\"\\w+…|…\", \"\", text) \n",
    "    text = re.sub(r\"(?<=\\w)-(?=\\w)\", \" \", text) \n",
    "    text = re.sub(\n",
    "        f\"[{re.escape(string.punctuation)}]\", \"\", text\n",
    "    )  # Remove punctuation\n",
    "    text = re.sub('[^a-z\\s]+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq(sent):\n",
    "    arr = np.zeros(26)\n",
    "    for letter in sent:\n",
    "        if 97 <= ord(letter) <= 122:\n",
    "            # print(\"here\")\n",
    "            arr[ord(letter) - 97] += 1 \n",
    "    #arr[::-1].sort()\n",
    "    \n",
    "    return arr / np.linalg.norm(arr) #normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['description_cleaned'] = df['description'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['histograms'] = df['description_cleaned'].apply(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = 2\n",
    "def mbkmeans_clusters(X, k):\n",
    "    km = KMeans(n_clusters=k).fit(X)\n",
    "    return km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[ df.histograms.map(lambda x: ~np.isnan(x).any()), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = mbkmeans_clusters(\n",
    "\tX=list(df['histograms'].values),\n",
    "    k=clusters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cluster\"] = cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hin = df[df.cluster == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>description_cleaned</th>\n",
       "      <th>histograms</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sir mai virendra pratap singh uttar pradesh ka...</td>\n",
       "      <td>sir mai virendra pratap singh uttar pradesh ka...</td>\n",
       "      <td>[0.6729258182937208, 0.08504007593821747, 0.02...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Respected Sir I am a B.tech student. I follows...</td>\n",
       "      <td>respected sir i am a btech student i follows y...</td>\n",
       "      <td>[0.541687646512873, 0.07194289055249095, 0.084...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Shriman Narendra Modi ji Mera Naam Anil Kumar ...</td>\n",
       "      <td>shriman narendra modi ji mera naam anil kumar ...</td>\n",
       "      <td>[0.6543680961547901, 0.07634294455139218, 0.06...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Seva Mein Shriman mananiy Pradhanmantri mahoda...</td>\n",
       "      <td>seva mein shriman mananiy pradhanmantri mahoda...</td>\n",
       "      <td>[0.6189839237896101, 0.04609454751624756, 0.0,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Good evening sir,sir mere dwara ccsu meerut ma...</td>\n",
       "      <td>good evening sirsir mere dwara ccsu meerut mai...</td>\n",
       "      <td>[0.5848016933547654, 0.06961924920890064, 0.08...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99979</th>\n",
       "      <td>Shri Pradhanmantri Narendra Modi ji Meri aapse...</td>\n",
       "      <td>shri pradhanmantri narendra modi ji meri aapse...</td>\n",
       "      <td>[0.6979755597443041, 0.08211477173462402, 0.0,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99980</th>\n",
       "      <td>Modi ji up me rojgar ka bhaout bura haal hai a...</td>\n",
       "      <td>modi ji up me rojgar ka bhaout bura haal hai a...</td>\n",
       "      <td>[0.6294033330928512, 0.13112569439434402, 0.10...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99982</th>\n",
       "      <td>sar mein ek bahut chhote se gaon ka Rahane wal...</td>\n",
       "      <td>sar mein ek bahut chhote se gaon ka rahane wal...</td>\n",
       "      <td>[0.6306570524869749, 0.07459384491781423, 0.10...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99988</th>\n",
       "      <td>Sirr plzz request aa ssc gd 2018 all medical f...</td>\n",
       "      <td>sirr plzz request aa ssc gd  all medical fit j...</td>\n",
       "      <td>[0.6945372265949069, 0.056313829183370835, 0.1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>M pichle 3 year se preparation kr rhi hu.. Par...</td>\n",
       "      <td>m pichle  year se preparation kr rhi hu par ss...</td>\n",
       "      <td>[0.608480883005074, 0.08392839765587227, 0.076...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12091 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             description  \\\n",
       "3      Sir mai virendra pratap singh uttar pradesh ka...   \n",
       "5      Respected Sir I am a B.tech student. I follows...   \n",
       "18     Shriman Narendra Modi ji Mera Naam Anil Kumar ...   \n",
       "24     Seva Mein Shriman mananiy Pradhanmantri mahoda...   \n",
       "26     Good evening sir,sir mere dwara ccsu meerut ma...   \n",
       "...                                                  ...   \n",
       "99979  Shri Pradhanmantri Narendra Modi ji Meri aapse...   \n",
       "99980  Modi ji up me rojgar ka bhaout bura haal hai a...   \n",
       "99982  sar mein ek bahut chhote se gaon ka Rahane wal...   \n",
       "99988  Sirr plzz request aa ssc gd 2018 all medical f...   \n",
       "99999  M pichle 3 year se preparation kr rhi hu.. Par...   \n",
       "\n",
       "                                     description_cleaned  \\\n",
       "3      sir mai virendra pratap singh uttar pradesh ka...   \n",
       "5      respected sir i am a btech student i follows y...   \n",
       "18     shriman narendra modi ji mera naam anil kumar ...   \n",
       "24     seva mein shriman mananiy pradhanmantri mahoda...   \n",
       "26     good evening sirsir mere dwara ccsu meerut mai...   \n",
       "...                                                  ...   \n",
       "99979  shri pradhanmantri narendra modi ji meri aapse...   \n",
       "99980  modi ji up me rojgar ka bhaout bura haal hai a...   \n",
       "99982  sar mein ek bahut chhote se gaon ka rahane wal...   \n",
       "99988  sirr plzz request aa ssc gd  all medical fit j...   \n",
       "99999  m pichle  year se preparation kr rhi hu par ss...   \n",
       "\n",
       "                                              histograms  cluster  \n",
       "3      [0.6729258182937208, 0.08504007593821747, 0.02...        1  \n",
       "5      [0.541687646512873, 0.07194289055249095, 0.084...        1  \n",
       "18     [0.6543680961547901, 0.07634294455139218, 0.06...        1  \n",
       "24     [0.6189839237896101, 0.04609454751624756, 0.0,...        1  \n",
       "26     [0.5848016933547654, 0.06961924920890064, 0.08...        1  \n",
       "...                                                  ...      ...  \n",
       "99979  [0.6979755597443041, 0.08211477173462402, 0.0,...        1  \n",
       "99980  [0.6294033330928512, 0.13112569439434402, 0.10...        1  \n",
       "99982  [0.6306570524869749, 0.07459384491781423, 0.10...        1  \n",
       "99988  [0.6945372265949069, 0.056313829183370835, 0.1...        1  \n",
       "99999  [0.608480883005074, 0.08392839765587227, 0.076...        1  \n",
       "\n",
       "[12091 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_total_hashes(sent, tokenizer):\n",
    "    tot = 0\n",
    "    lst  = tokenizer.tokenize(sent)\n",
    "    # print(lst)\n",
    "    for word in lst:\n",
    "        tot += word.count('#')\n",
    "    return tot\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_total_hashes_m2m(sent, tokenizer = tokenizer_m2m):\n",
    "    tot = 0\n",
    "    lst  = tokenizer.tokenize(sent)\n",
    "    # print(lst)\n",
    "    for i in range(len(lst)):\n",
    "        t = list(lst[i])\n",
    "        \n",
    "        if t[0] == '▁':\n",
    "            t.pop(0)\n",
    "        else: \n",
    "            t[0] = '#'+'#'+t[0]\n",
    "        lst[i] = ''.join(t)   \n",
    "        \n",
    "    # print(lst)\n",
    "    for word in lst:\n",
    "        tot += word.count('#')\n",
    "    return tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_hash_to_character_ratio(word):\n",
    "    c_hash, c_char = 0,0\n",
    "    for c in word:\n",
    "        if c=='#':\n",
    "            c_hash+=1\n",
    "        elif 97<=ord(c)<=122:\n",
    "            c_char+=1\n",
    "    return round(c_hash/c_char, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hash_to_character_ratio_sent(sent, tokenizer):\n",
    "    lst  = tokenizer.tokenize(sent)\n",
    "    \n",
    "    dct = {}\n",
    "    for word in lst:\n",
    "        try:\n",
    "            dct[count_hash_to_character_ratio(word)]+=1\n",
    "        except:\n",
    "            dct[count_hash_to_character_ratio(word)] = 0\n",
    "            dct[count_hash_to_character_ratio(word)]+=1\n",
    "    \n",
    "    return {round(k,2):v for k,v in sorted(dct.items(), key=lambda x: x[0], reverse=True)}, len(lst)\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hash_to_character_ratio_sent_m2m(sent, tokenizer):\n",
    "    lst  = tokenizer.tokenize(sent)\n",
    "    for i in range(len(lst)):\n",
    "        t = list(lst[i])\n",
    "        \n",
    "        if t[0] == '▁':\n",
    "            t.pop(0)\n",
    "        else: \n",
    "            t[0] = '#'+'#'+t[0]\n",
    "        lst[i] = ''.join(t)\n",
    "    dct = {}\n",
    "    for word in lst:\n",
    "        try:\n",
    "            dct[count_hash_to_character_ratio(word)]+=1\n",
    "        except:\n",
    "            dct[count_hash_to_character_ratio(word)] = 0\n",
    "            dct[count_hash_to_character_ratio(word)]+=1\n",
    "    \n",
    "    return {round(k,2):v for k,v in sorted(dct.items(), key=lambda x: x[0], reverse=True)}, len(lst)\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({1.0: 7, 0.67: 4, 0.4: 2, 0.0: 37}, 50)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_hash_to_character_ratio_sent_m2m(s, tokenizer_m2m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"sar mein ek bahut chhote se gaon ka rahane wala hun aur ek meri chhoti si kirana dukaan hai main usi par apni dukaan chalata hun aur maine digital india for all ka if liya tha aur\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_salesken = AutoTokenizer.from_pretrained(\"salesken/similarity-eng-hin_latin\")\n",
    "tokenizer_hing_bert = AutoTokenizer.from_pretrained(\"l3cube-pune/hing-bert\")\n",
    "tokenizer_hing_mbert = AutoTokenizer.from_pretrained(\"l3cube-pune/hing-mbert\")\n",
    "tokenizer_m2m = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\", src_lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_grievances = list(hin.description_cleaned.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12091"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hindi_grievances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_grievances = [g for g in hindi_grievances if 100 <len(g.split()) < 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4471"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hindi_grievances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_total_hashes(s, tokenizer_salesken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_grievances_20 = random.sample(hindi_grievances, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = hindi_grievances_20[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_total_hashes_m2m(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df = pd.DataFrame(columns=['#s', 2.0, 1.0, 0.0])\n",
    "for sent in hindi_grievances_20:\n",
    "    # print(sent)\n",
    "    tmp = []\n",
    "    l1, sz = calculate_hash_to_character_ratio_sent(sent, tokenizer_hing_bert)\n",
    "    \n",
    "    l1[99.99] = count_total_hashes(sent, tokenizer_hing_bert)\n",
    "    # l1[199.99] = sz\n",
    "    l1 = {k: v / sz for k, v in l1.items()}\n",
    "    l1 = {k:v for k,v in sorted(l1.items(), key=lambda x: x[0], reverse=True)}\n",
    "    tmp.append(l1)\n",
    "    \n",
    "    \n",
    "    l2,sz = calculate_hash_to_character_ratio_sent(sent, tokenizer_salesken)\n",
    "    \n",
    "    l2[99.99] = count_total_hashes(sent, tokenizer_salesken)\n",
    "    # l2[199.99] = sz\n",
    "    l2 = {k: v / sz for k, v in l2.items()}\n",
    "    l2 = {k:v for k,v in sorted(l2.items(), key=lambda x: x[0], reverse=True)}\n",
    "    tmp.append(l2)\n",
    "    \n",
    "    l3, sz = calculate_hash_to_character_ratio_sent(sent, tokenizer_hing_mbert)\n",
    "    \n",
    "    l3[99.99] = count_total_hashes(sent, tokenizer_hing_mbert)\n",
    "    # l3[199.99] =  sz\n",
    "    l3 = {k: v / sz for k, v in l3.items()}\n",
    "    l3 = {k:v for k,v in sorted(l3.items(), key=lambda x: x[0], reverse=True)}\n",
    "    tmp.append(l3)\n",
    "    \n",
    "    l4, sz = calculate_hash_to_character_ratio_sent_m2m(sent, tokenizer_m2m)\n",
    "    l4[99.99] = count_total_hashes_m2m(sent, tokenizer_m2m)\n",
    "    # l4[99.99] = count_total_hashes_m2m(sent)\n",
    "    l4 = {k: v / sz for k, v in l4.items()}\n",
    "    l4 = {k:v for k,v in sorted(l4.items(), key=lambda x: x[0], reverse=True)}\n",
    "    tmp.append(l4)\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(tmp)\n",
    "    lst = list(df.columns)\n",
    "    # lst[0] = \"Total_tokens\"\n",
    "    lst[0] = \"#s\"\n",
    "    df.columns = lst\n",
    "    df.fillna(0, inplace=True)\n",
    "    # print(df, '\\n')\n",
    "    # print(lst)\n",
    "    df =  df[['#s', 2.0, 1.0, 0.5, 0.0]]\n",
    "    if len(mean_df)==0:\n",
    "        mean_df = df\n",
    "    else:\n",
    "        mean_df = mean_df.add(df)\n",
    "    \n",
    "    # print(df.columns)\n",
    "    \n",
    "mean_df = (mean_df/500).round(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df.index = [\"tokenizer_hing_bert\", \"tokenizer_salesken\", \"tokenizer_hing_mbert\", \"tokenizer_m2m\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#s</th>\n",
       "      <th>2.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokenizer_hing_bert</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer_salesken</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer_hing_mbert</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer_m2m</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        #s   2.0   1.0   0.5   0.0\n",
       "tokenizer_hing_bert   0.82  0.13  0.17  0.02  0.59\n",
       "tokenizer_salesken    0.58  0.10  0.09  0.03  0.71\n",
       "tokenizer_hing_mbert  0.79  0.12  0.15  0.03  0.61\n",
       "tokenizer_m2m         0.72  0.04  0.16  0.04  0.64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #s should be minimum\n",
    "- 2.0 ratio means tokens with single letters, so should be minimum\n",
    "- 1.0 ratio should be minimum, 2 hashes and 2 letters in a token\n",
    "- 0.5 ratio should be maximum, 2 hashes and 4 letters in a token\n",
    "- 0.0 should be maximum as these are fully recognised tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sar mein ek bahut chhote se gaon ka rahane wala hun aur ek meri chhoti si kirana dukaan hai main usi par apni dukaan chalata hun aur maine digital india for all ka if liya tha aur'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer_salesken \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39msalesken/similarity-eng-hin_latin\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m tokenizer_hing_bert \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39ml3cube-pune/hing-bert\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m tokenizer_hing_mbert \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39ml3cube-pune/hing-mbert\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer_salesken = AutoTokenizer.from_pretrained(\"salesken/similarity-eng-hin_latin\")\n",
    "tokenizer_hing_bert = AutoTokenizer.from_pretrained(\"l3cube-pune/hing-bert\")\n",
    "tokenizer_hing_mbert = AutoTokenizer.from_pretrained(\"l3cube-pune/hing-mbert\")\n",
    "tokenizer_m2m = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\", src_lang=\"en\")\n",
    "\n",
    "def count_chars(word):\n",
    "    c = 0\n",
    "    for w in word:\n",
    "        if 97<=ord(w)<=122:\n",
    "            c+=1\n",
    "    return c\n",
    "\n",
    "def count_freq(s, tokeniser):\n",
    "    d = dict()\n",
    "    lst = tokeniser.tokenize(s)\n",
    "    counts = sorted([count_chars(w) for w in lst])\n",
    "    return counts\n",
    "\n",
    "def entropy(labels, base=None):\n",
    "\n",
    "  n_labels = len(labels)\n",
    "\n",
    "  if n_labels <= 1:\n",
    "    return 0\n",
    "\n",
    "  value,counts = np.unique(labels, return_counts=True)\n",
    "  probs = counts / n_labels\n",
    "  n_classes = np.count_nonzero(probs)\n",
    "\n",
    "  if n_classes <= 1:\n",
    "    return 0\n",
    "\n",
    "  ent = 0.\n",
    "\n",
    "  # Compute entropy\n",
    "  base = np.e if base is None else base\n",
    "  for i in probs:\n",
    "    ent -= i * log(i, base)\n",
    "\n",
    "  return ent\n",
    "\n",
    "\n",
    "tokenizer_hing_bert_sum = 0\n",
    "tokenizer_hing_mbert_sum = 0\n",
    "tokenizer_m2m_sum = 0\n",
    "tokenizer_salesken_sum = 0\n",
    "\n",
    "n = len(hindi_grievances)\n",
    "\n",
    "for sent in hindi_grievances:\n",
    "    tokenizer_m2m_sum += entropy(count_freq(s, tokenizer_m2m), 2)\n",
    "    tokenizer_hing_bert_sum += entropy(count_freq(s, tokenizer_hing_bert), 2)\n",
    "    tokenizer_hing_mbert_sum += entropy(count_freq(s, tokenizer_hing_mbert), 2)\n",
    "    tokenizer_salesken_sum += entropy(count_freq(s, tokenizer_salesken), 2)\n",
    "\n",
    "out = dict()\n",
    "\n",
    "out[\"tokenizer_hing_bert\"] = tokenizer_hing_bert_sum/n\n",
    "out[\"tokenizer_hing_mbert\"] = tokenizer_hing_mbert_sum/n\n",
    "out[\"tokenizer_m2m_sum\"] = tokenizer_m2m_sum/n\n",
    "out[\"tokenizer_salesken_sum\"] = tokenizer_salesken_sum/n\n",
    "\n",
    "out = {k:v for k,v in sorted(out.items(), key=lambda x: x[1])}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9701380294277913\n",
      "1.9941253738291251\n",
      "2.025290531126644\n",
      "2.2281789842251594\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokenizer_m2m_sum': 1.9701380294279907,\n",
       " 'tokenizer_hing_bert': 1.9941253738290246,\n",
       " 'tokenizer_hing_mbert': 2.025290531126495,\n",
       " 'tokenizer_salesken_sum': 2.228178984225302}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Mar 10 2023, 20:16:38) \n[Clang 14.0.3 (clang-1403.0.22.14.1)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
