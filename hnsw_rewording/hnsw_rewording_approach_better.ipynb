{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from math import log\n",
    "import os\n",
    "import random\n",
    "import hnswlib\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "import pickle\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "print(sys.setrecursionlimit(5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_counts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = str(text).lower().strip() \n",
    "    text = re.sub(\"(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*?)\", \"\", text)\n",
    "    text = re.sub('[^a-z\\s]+', \"\", text)\n",
    "    text = text.translate (str.maketrans('', '', string.punctuation))\n",
    "      # Remove multiple spaces in content\n",
    "    text = re.sub('[!()-[]{};:\\'\"\\,<>./?@#$%^&*_~â€“]+', '', text)\n",
    "    # text = re.sub(\"\\d+\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the function performs various text cleaning tasks, such as converting to lowercase, removing URLs, non-alphabetic characters, and punctuation marks, and reducing multiple spaces to a single space, to obtain a cleaned version of the input text suitable for further processing or analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_roman_words = Counter()\n",
    "pref = \"/home/installer/ps/long_term/data/\"\n",
    "\n",
    "for file in os.listdir(\"/home/installer/ps/long_term/data/\"):\n",
    "    if not file.endswith(\"csv\"):\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(\"/home/installer/ps/long_term/data/\", file)\n",
    "    print(file, \"here\")\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        df = pd.read_csv(f, usecols=[0, 21], header=None, low_memory=False)\n",
    "        df.columns = ['reg_no', 'subject_content']\n",
    "        df['subject_content_cleaned'] = df['subject_content'].apply(clean_text)\n",
    "\n",
    "        df['subject_content_cleaned'] = df['subject_content'].apply(clean_text)\n",
    "        for sent in df.subject_content_cleaned.values:\n",
    "            res = re.findall(r'\\w+', sent)\n",
    "            all_roman_words.update(res)\n",
    "        print(file, \"Size of updated set\", str(len(all_roman_words)))\n",
    "\n",
    "    # print(file, \"Size of updated set\", str(len(all_roman_words)))\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is reading multiple CSV files from a directory, extracting specific columns ('reg_no' and 'subject_content'), cleaning the text data in the 'subject_content' column using a function called 'clean_text', and then extracting all the individual words (excluding punctuation) from the cleaned text using regular expressions. These words are then added to a counter called 'all_roman_words', which keeps track of the frequency of each word across all the files. The code prints the name of each file being processed and the size of the updated 'all_roman_words' counter after processing each file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('all_roman_words.pkl', 'wb') as f:\n",
    "    pickle.dump(all_roman_words, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "len(all_roman_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memo(f):\n",
    "    \"Memoize function f, whose args must all be hashable.\"\n",
    "    cache = {}\n",
    "    def fmemo(*args):\n",
    "        if args not in cache:\n",
    "            cache[args] = f(*args)\n",
    "        return cache[args]\n",
    "    fmemo.cache = cache\n",
    "    return fmemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(len(w) for w in all_roman_words.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pword(word, N=sum(all_roman_words.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return all_roman_words[word] / N\n",
    "\n",
    "def Pwords(words):\n",
    "    \"Probability of words, assuming each word is independent of others.\"\n",
    "    return product(Pword(w) for w in words)\n",
    "\n",
    "def product(nums):\n",
    "    \"Multiply the numbers together.  (Like `sum`, but with multiplication.)\"\n",
    "    result = 1\n",
    "    for x in nums:\n",
    "        result *= x\n",
    "    return result\n",
    "\n",
    "def splits(text, start=0, L=max_len):\n",
    "    \"Return a list of all (first, rest) pairs; start <= len(first) <= L.\"\n",
    "    return [(text[:i], text[i:]) \n",
    "            for i in range(start, min(len(text), L)+1)]\n",
    "    \n",
    "@memo\n",
    "def segment(text):\n",
    "    \"Return a list of words that is the most probable segmentation of text.\"\n",
    "    if not text: \n",
    "        return []\n",
    "    else:\n",
    "        candidates = ([first] + segment(rest) \n",
    "                      for (first, rest) in splits(text, 1))\n",
    "        return max(candidates, key=Pwords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The segment(text) function utilizes dynamic programming to find the most probable segmentation of a given input text. It generates all possible splits, recursively calculates probabilities using Pwords(words) function, and returns the segmentation with the highest probability. It's a recursive implementation of Viterbi algorithm used in NLP tasks like POS tagging and named entity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increase_count(words, k):\n",
    "    result = []\n",
    "    for word in words:\n",
    "        result += [word] * k\n",
    "    return result\n",
    "\n",
    "\n",
    "for k in list(all_roman_words):\n",
    "    try:\n",
    "        tmp = segment(k)\n",
    "        if (len(tmp)>1):\n",
    "            tmp = increase_count(tmp, all_roman_words[k])\n",
    "            all_roman_words.update(tmp)\n",
    "            del all_roman_words[k]\n",
    "    except:\n",
    "        print(k)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code seems to be iterating through a dictionary called all_roman_words and attempting to segment each word into its constituent parts using a function called segment. If the resulting list has more than one element, it multiplies that list by a factor of all_roman_words[k] using a function called increase_count. It then updates the dictionary with the new segmented and multiplied list and deletes the original word from the dictionary. Overall, the code appears to be manipulating a dictionary of Roman words, breaking them down into their components, and multiplying the components based on the frequency of the original word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # del all_roman_words[k]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_roman_words.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_roman_words_segmented.pkl', 'wb') as f:\n",
    "    pickle.dump(all_roman_words, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/installer/ps/hnsw_rewording/all_roman_words_segmented.pkl', 'rb') as f:\n",
    "    all_roman_words = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_roman_words.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_hnsw_index(features, ef=100, M=32):\n",
    "    num_elements = len(features)\n",
    "    labels_index = np.arange(num_elements)\n",
    "    EMBEDDING_SIZE = len(features[0])\n",
    "    \n",
    "    p = hnswlib.Index(space='l2', dim=EMBEDDING_SIZE)# possible space options are l2, cosine or ip\n",
    "    p.init_index(max_elements=num_elements, ef_construction=ef, M=M)\n",
    "    int_labels = p.add_items(features, labels_index)\n",
    "    p.set_ef(-1) \n",
    "    return p\n",
    "\n",
    "def freq(sent):\n",
    "    arr = np.zeros(26)\n",
    "    for letter in sent:\n",
    "        if 97 <= ord(letter) <= 122:\n",
    "            # print(\"here\")\n",
    "            arr[ord(letter) - 97] += 1 \n",
    "    #arr[::-1].sort()\n",
    "    \n",
    "    return arr / np.linalg.norm(arr) #normalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first function, \"fit_hnsw_index,\" trains an HNSW index using a set of input features. The index is constructed using the \"l2\" space and is initialized with specified values of \"ef\" and \"M.\" The function returns the trained index. The second function, \"freq,\" calculates the frequency of each alphabet in a given input sentence and returns a normalized array of these frequencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_roman_words.keys(), columns= ['word'])\n",
    "df['histograms'] = df['word'].apply(freq)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('df_histograms.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/installer/ps/hnsw_rewording/df_histograms.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 50 #nearest neighbours you wish to find for each grievance\n",
    "l2_param = 0.001 #threshold for l2 distance\n",
    "min_docs = 1 #minimum number of grievances in a set to call it a bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [item.tolist() for item in df['histograms'].values]\n",
    "\n",
    "p = fit_hnsw_index(X, ef=K*10)\n",
    "\n",
    "def find_nearest(index, word, k=50):\n",
    "    ann_neighbor_indices, ann_distances = index.knn_query(freq(word), k)\n",
    "    out = np.array([df.loc[i, \"word\"].values for i in ann_neighbor_indices])\n",
    "    ann_distances = np.array(ann_distances)\n",
    "    return out\n",
    "\n",
    "primary_dict = {w: find_nearest(p, w) for w in all_roman_words.keys()}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is creating a primary dictionary of nearest neighbors for each word in a set of input data. The input data consists of histograms stored in a Pandas DataFrame. First, the histograms are converted to a list of lists format. Then, an HNSW index is fit to the list of histograms. Finally, a function is defined that finds the nearest neighbors for a given word based on the HNSW index. The nearest neighbors are returned as an array of words, with the distance to each neighbor also provided. The primary dictionary is created by calling this function for each word in a pre-defined set of words, creating a dictionary where each word maps to its nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('primary_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(primary_dict, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/installer/ps/hnsw_rewording/primary_dict.pkl', 'rb') as f:\n",
    "    primary_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def known(words, lwords): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in lwords)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "\n",
    "def candidates(word, lwords): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return( known(edits1(word), lwords) or known(edits2(word), lwords) or known(word, lwords) or [word])\n",
    "\n",
    "\n",
    "def P(word, N=sum(all_roman_words.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return all_roman_words[word] / N\n",
    "\n",
    "def correction(word, lwords): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word, lwords), key=P)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines functions for spelling correction using the edit distance algorithm. It generates a list of possible spelling corrections for a given word, ranks them by their probability of appearing in a dictionary, and returns the correction with the highest probability. The code uses a dictionary of known words to calculate probabilities and edit distance functions to generate possible corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tmp.pkl', 'wb') as f:\n",
    "    pickle.dump(tmp, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/installer/ps/hnsw_rewording/tmp.pkl', 'rb') as f:\n",
    "    tmp = pickle.load(f)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = {key: candidates(key, primary_dict[key]) for key in primary_dict.keys()}\n",
    "\n",
    "\n",
    "english_words = set()\n",
    "with open(\"word.list\") as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        english_words.add(line)\n",
    "        \n",
    "letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "for l in letters:\n",
    "    if l=='a':\n",
    "        continue\n",
    "    try:\n",
    "        # del WORDS[l]\n",
    "        del tmp[l]\n",
    "    except:\n",
    "        print(l, \"doesn't exist\")\n",
    "        \n",
    "def build_word_mapping(dictionary):\n",
    "    word_mapping = {}\n",
    "    for key, values in dictionary.items():\n",
    "        for value in values:\n",
    "            words = value.split()\n",
    "            for word in words:\n",
    "                if word not in word_mapping:\n",
    "                    word_mapping[word] = key\n",
    "    return word_mapping\n",
    "\n",
    "word_mapping = build_word_mapping(tmp)\n",
    "\n",
    "def correct_word(word, word_mapping):\n",
    "    corrected_word = word\n",
    "    if word in word_mapping:\n",
    "        corrected_word = word_mapping[word]\n",
    "    return corrected_word"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code reads a list of English words and creates a dictionary mapping letters to sets of words containing that letter. It then removes all sets of words containing the letter 'a'. Next, it creates a word mapping dictionary by assigning each word to its corresponding set of letters, and finally defines a function to correct words based on this mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_mapping.pkl', 'wb') as f:\n",
    "    pickle.dump(word_mapping, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_word(\"kya\", word_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Open the file in binary mode\n",
    "with open('trans_hindi.pickle', 'rb') as f:\n",
    "    # Load the object from the file\n",
    "    trans_hindi = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_hindi = set(trans_hindi.values())\n",
    "len(trans_hindi)\n",
    "\n",
    "final_wording = dict()\n",
    "\n",
    "# Function to get the correct word using word_mapping\n",
    "def get_correct_word(word, mapping):\n",
    "    while word != mapping.get(word, word):\n",
    "        word = mapping[word]\n",
    "    return word\n",
    "\n",
    "for v in word_mapping.keys():\n",
    "    if v not in english_words and v not in trans_hindi:\n",
    "        w = get_correct_word(v, word_mapping)\n",
    "        if w in final_wording:\n",
    "            final_wording[v] = final_wording[w]\n",
    "        else:\n",
    "            final_wording[v] = w\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code first creates a set of Hindi translations, and then creates an empty dictionary to store the final word mapping. It defines a function to recursively look up the correct word using the word mapping. It then iterates over the keys in the word mapping, and for each key that is not an English word or a Hindi translation, it uses the get_correct_word function to find the correct word, and stores it in the final_wording dictionary. If the correct word is already in the dictionary, it maps the current word to the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_wording\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('final_wording.pkl', 'wb') as f:\n",
    "    pickle.dump(final_wording, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16 (default, Mar  2 2023, 03:21:46) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db07cf3586e4a3d517105e23490166b0186d57892d50b74d9e1d6eda510229fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
