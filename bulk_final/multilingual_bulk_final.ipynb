{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reference: https://pub.towardsai.net/knn-k-nearest-neighbors-is-dead-fc16507eb3e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "import hnswlib\n",
    "import _pickle as cPickle\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess textual data by performing the following steps:\n",
    "    1. Remove punctuation\n",
    "    2. Convert text to lowercase\n",
    "    3. Remove digits and special characters\n",
    "    4. Remove extra whitespaces\n",
    "    \"\"\"\n",
    "    if not isinstance(text, (str, bytes)):\n",
    "        return text\n",
    "    \n",
    "    # remove links\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove digits and special characters\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference function to build the index for HNSW\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_hnsw_index(features, ef=100, M=16):\n",
    "    # Get the number of elements in the feature set\n",
    "    num_elements = len(features)\n",
    "    # Create an array of labels for the elements in the feature set\n",
    "    labels_index = np.arange(num_elements)\n",
    "\n",
    "    # Get the dimensionality of the features\n",
    "    EMBEDDING_SIZE = len(features[0])\n",
    "\n",
    "    # Initialize the hnsw index with the specified parameters\n",
    "    p = hnswlib.Index(space='l2', dim=EMBEDDING_SIZE)\n",
    "    p.init_index(max_elements=num_elements, ef_construction=ef, M=M)\n",
    "\n",
    "    # Add the features to the index and get the integer labels\n",
    "    int_labels = p.add_items(features, labels_index)\n",
    "\n",
    "    # Set the ef parameter for the index\n",
    "    p.set_ef(ef) \n",
    "\n",
    "    # Return the hnsw index\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"embeddings_dict.pkl\", \"rb\") as f:\n",
    "    embeddings_dict = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_embedding(sentence, embeddings_dict = embeddings_dict):\n",
    "    # split the sentence into tokens\n",
    "    tokens = re.findall(r\"[\\w']+|[^\\w\\s]\", sentence, re.UNICODE)\n",
    "    \n",
    "    # create a list to store the embeddings for each token\n",
    "    embeddings = [embeddings_dict[lang_code][token] for token in tokens for lang_code in embeddings_dict.keys() if token in embeddings_dict[lang_code]]\n",
    "    \n",
    "    # create a 300d vector by taking the average of the embeddings\n",
    "    vector = np.mean(np.array(embeddings), axis=0) if embeddings else np.zeros(300)\n",
    "    \n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10000000000000005"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_threshold = 0.995\n",
    "l2_threshold = math.sqrt(2 * (1 - cosine_threshold))\n",
    "l2_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function named \"bulkify\" that takes three arguments:\n",
    "# 1. \"df\": a pandas DataFrame containing two columns named \"reg_no\" and \"subject_content\"\n",
    "# 2. \"l2_threshold\" (default value: 0.005): a float representing the L2 threshold distance\n",
    "# 3. \"min_docs\" (default value: 25): an integer representing the minimum number of documents in a bulk\n",
    "def bulkify(df, l2_threshold=l2_threshold, min_docs=5):\n",
    "    # Set K to the minimum of the length of the DataFrame and 1000\n",
    "    K=min(len(df)-1, 1000)\n",
    "    \n",
    "    # Record the start time\n",
    "    start = time.time()\n",
    "    \n",
    "    # Rename the columns of the DataFrame to \"reg_no\" and \"subject_content\"\n",
    "    df.columns = ['reg_no', 'subject_content']\n",
    "    \n",
    "    # Apply a function called \"clean_text\" to the \"subject_content\" column and store the result in a new column named \"subject_content_cleaned\"\n",
    "    df['subject_content_cleaned'] = df['subject_content'].apply(clean_text)\n",
    "    \n",
    "    # Replace empty strings in the \"subject_content_cleaned\" column with NaN values\n",
    "    df['subject_content_cleaned'].replace('', np.nan, inplace=True)\n",
    "    \n",
    "    # Remove rows with NaN values in the \"subject_content_cleaned\" column\n",
    "    df.dropna(subset=['subject_content_cleaned'], inplace=True)\n",
    "    \n",
    "    # Reset the index of the DataFrame\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Generate embeddings for each text in the \"subject_content_cleaned\" column and stack them vertically\n",
    "    emb = np.vstack([generate_embedding(s) for s in df['subject_content_cleaned']])\n",
    "\n",
    "    # Print the elapsed time since the start of the function\n",
    "    print(f'Embedding Time: {time.time() - start}')\n",
    "\n",
    "    # Fit a Hierarchical Navigable Small World (HNSW) index to the embeddings with an exploration factor (ef) of K*10\n",
    "    p = fit_hnsw_index(emb, ef=K*10)\n",
    "    \n",
    "    # Find the K nearest neighbors and their distances for each embedding using the HNSW index\n",
    "    ann_neighbor_indices, ann_distances = p.knn_query(emb, K)\n",
    "    \n",
    "    # Print the elapsed time since the start of the function\n",
    "    print(f'HNSW Time: {time.time() - start}')\n",
    "\n",
    "    # Calculate the L2 distance threshold based on the given cosine similarity threshold (not used in this implementation)\n",
    "    # cos_sim_threshold = cos_sim_param\n",
    "    # l2_threshold = np.sqrt(2 * (1 - cos_sim_threshold))\n",
    "    \n",
    "    # Create a dictionary where each key is an index of an embedding and its value is a list of indices of its nearest neighbors within the L2 distance threshold\n",
    "    primary_dict = {i: [k for k, v in zip(indices, distances) if k != i and v < l2_threshold]\n",
    "                    for i, (indices, distances) in enumerate(zip(ann_neighbor_indices, ann_distances))}\n",
    "\n",
    "    # Create an empty list and set to store all the bulk sets and indices that have been added to a bulk\n",
    "    bulk_all = []\n",
    "    bulk_set = set()\n",
    "\n",
    "    # loop through the primary documents in the primary_dict\n",
    "    for i in range(len(primary_dict)):\n",
    "        # skip documents already included in a bulk\n",
    "        if i in bulk_set:\n",
    "            continue\n",
    "        # add the current primary document to the tmp_docs list\n",
    "        tmp_docs = [df.loc[i, 'reg_no']]\n",
    "        # loop through the similar documents for the current primary document\n",
    "        for k in sorted(primary_dict[i]):\n",
    "            # skip documents already included in a bulk\n",
    "            if k in bulk_set:\n",
    "                continue\n",
    "            # add the current similar document to the tmp_docs list\n",
    "            tmp_docs.append(df.loc[k, 'reg_no'])\n",
    "            # add the current similar document to the bulk_set\n",
    "            bulk_set.add(k)\n",
    "        # if the tmp_docs list contains more than min_docs documents, add it to the bulk_all list\n",
    "        if len(tmp_docs) >= min_docs:\n",
    "            bulk_all.append(tmp_docs)\n",
    "\n",
    "    # initialize a dictionary to store the bulk status for each document\n",
    "    out = {reg: \"\" for reg in df['reg_no'].values}\n",
    "    # print the length of the dictionary\n",
    "    print(len(out))\n",
    "\n",
    "    # Loop through each list in the input list of lists\n",
    "    for i in range(len(bulk_all)):\n",
    "        \n",
    "        # Get the current list\n",
    "        curr_bulk = bulk_all[i]\n",
    "        \n",
    "        # Sort the current list in ascending order\n",
    "        curr_bulk = sorted(curr_bulk)\n",
    "        \n",
    "        # Get the first element (minimum value) of the sorted list\n",
    "        first_id = curr_bulk[0]\n",
    "        \n",
    "        # Loop through each element in the current list\n",
    "        for r in curr_bulk:\n",
    "            \n",
    "            # Set the output for the current element to a tuple of the first element\n",
    "            # in the sorted list (first_id) and the length of the current list (len(curr_bulk))\n",
    "            out[r] = (first_id, len(curr_bulk))\n",
    "    \n",
    "    # Print the total time elapsed during processing\n",
    "    print(f'Total Time: {time.time() - start}')\n",
    "    \n",
    "    # Return the output dictionary\n",
    "    return out, bulk_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Time: 0.030653715133666992\n",
      "HNSW Time: 0.031068086624145508\n",
      "10\n",
      "Total Time: 0.03168296813964844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tmp = pd.read_csv(r\"bulk-campaign.csv\",\\\n",
    "    usecols=[0,7])\n",
    "df_tmp = df_tmp.sample(n = 10)\n",
    "\n",
    "out, bulk = bulkify(df_tmp)\n",
    "len(bulk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sen = \"ਮੈਂ ਆਜ office ਜਾਣਾ ਹੈ, ਸੋ let's ਸਟਾਰਟ ਕਰੀਏ the ਕੰਮ!\"\n",
    "# print(f\"Original sentence: {sen}\")\n",
    "# sen = clean_text(sen)\n",
    "# print(f\"Cleaned sentence: {sen}\")\n",
    "# emb = generate_embedding(sen)\n",
    "# print(f\"Length of embedding generated: {len(emb)}\")\n",
    "# print(f\"Preview of embedding:\\n {emb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "curr = bulk[0]\n",
    "print(len(curr))\n",
    "df_tmp[df_tmp.reg_no.isin(curr)][[\"reg_no\", \"subject_content\"]].to_csv(\"Bulk_example1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading page (1/2)\n",
      "Rendering (2/2)                                                    \n",
      "Done                                                               \n",
      "Loading page (1/2)\n",
      "Rendering (2/2)                                                    \n",
      "Done                                                               \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the dataframe to an HTML table\n",
    "html_table = df_tmp[df_tmp.reg_no.isin(curr)][[\"reg_no\", \"subject_content\"]].to_html(index=False)\n",
    "\n",
    "# use imgkit to convert the HTML table to an image\n",
    "imgkit.from_string(html_table, \"example1.png\")\n",
    "\n",
    "# convert the dataframe to an HTML table\n",
    "html_table = df_tmp[[\"reg_no\", \"subject_content\"]].to_html(index=False)\n",
    "\n",
    "# use imgkit to convert the HTML table to an image\n",
    "imgkit.from_string(html_table, \"data.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(Counter(out.keys())), Counter(out.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = Counter(out.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp.most_common(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues\n",
    "\n",
    "1. A few of the grievances are a code mix of different scripts. The above code can catch only those grievances which are in the same script\n",
    "2. With using Indic bert model accuracy is up but it is very slow\n",
    "3. If we wish to use fast text then accuracy dips but it is way faster\n",
    "4. Can we do something like translation first? Again it is slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db07cf3586e4a3d517105e23490166b0186d57892d50b74d9e1d6eda510229fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
